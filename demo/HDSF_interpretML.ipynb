{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example using HDFS data from samples folder. See also similar TB_samples.py \n",
    "#The files have been loaded from raw and processed to parquet file format for efficient storage.\n",
    "#This demonstrates how to work after you have completed the loader.\n",
    "\n",
    "#______________________________________________________________________________\n",
    "#Part 1 load libraries and setup paths. \n",
    "import sys\n",
    "import os\n",
    "#Ensure this always gets executed in the same location\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "os.chdir(script_dir)\n",
    "sys.path.append('..')\n",
    "import loglead.loaders.hdfs as load\n",
    "import loglead.enhancer as er\n",
    "import loglead.anomaly_detection as ad\n",
    "import polars as pl\n",
    "import random\n",
    "\n",
    "#our teams imports\n",
    "import pickle\n",
    "import numpy as np\n",
    "#import shap\n",
    "from interpret.glassbox import LogisticRegression\n",
    "from interpret import show\n",
    "from interpret import set_visualize_provider\n",
    "from interpret.provider import InlineProvider\n",
    "set_visualize_provider(InlineProvider())\n",
    "\n",
    "\n",
    "\n",
    "#Location of our sample data\n",
    "sample_data=\"../samples\"\n",
    "\n",
    "#_________________________________________________________________________________\n",
    "#Part 2 load data from sample file\n",
    "#Load HDFS from sample data\n",
    "df = pl.read_parquet(f\"{sample_data}/hdfs_events_2percent.parquet\")\n",
    "df_seqs_all = pl.read_parquet(f\"{sample_data}/hdfs_seqs_2percent.parquet\")\n",
    "df_seqs = df_seqs_all.head(1000)\n",
    "print(f\"Read HDFS 2% sample. Numbers of events is: {len(df)} and number of sequences is {len(df_seqs)}\")\n",
    "ano_count = df_seqs[\"anomaly\"].sum()\n",
    "print(f\"Anomaly count {ano_count}. Anomaly percentage in Sequences {ano_count/len(df_seqs)*100:.2f}%\")\n",
    "\n",
    "\n",
    "#_________________________________________________________________________________\n",
    "#Part 3 add enhanced reprisentations \n",
    "print(f\"\\nStarting enhancing all log events:\")\n",
    "enhancer = er.EventLogEnhancer(df)\n",
    "\n",
    "# For nicer printing a function to format series of elements as a list-like string\n",
    "def format_as_list(series):\n",
    "    elements = [str(item) for item in series]\n",
    "    return '[' + ', '.join(elements) + ']'\n",
    "#Pick a random line\n",
    "row_index = random.randint(0, len(df) - 1)\n",
    "\n",
    "\n",
    "print(f\"Original log message: {df['m_message'][row_index]}\")\n",
    "#Create some enhanced representations\n",
    "df = enhancer.normalize()\n",
    "print(f\"as normalized:        {df['e_message_normalized'][row_index]}\")\n",
    "df = enhancer.words()\n",
    "print(f\"as words:             {format_as_list(df['e_words'][row_index])}\")\n",
    "df = enhancer.trigrams()\n",
    "print(f\"as trigrams:          {format_as_list(df['e_trigrams'][row_index])}\")\n",
    "df = enhancer.parse_drain()\n",
    "print(f\"as Drain event id:    {df['e_event_id'][row_index]}\")\n",
    "df = enhancer.parse_spell()\n",
    "print(f\"as Spell event id:    {df['e_event_spell_id'][row_index]}\")\n",
    "df = enhancer.length()\n",
    "print(f\"event length chars:   {df['e_chars_len'][row_index]}\")\n",
    "print(f\"event length lines:   {df['e_lines_len'][row_index]}\")\n",
    "print(f\"event length words:   {df['e_words_len'][row_index]}\")\n",
    "\n",
    "\n",
    "\n",
    "#_________________________________________________________________________________\n",
    "#Part 4 Aggregate event level data to sequence level\n",
    "print(f\"\\nStarting aggregating log event info to log sequences:\")\n",
    "\n",
    "seqs_row_index = random.randint(0, len(df_seqs) - 1)\n",
    "seq_id = df_seqs['seq_id'][seqs_row_index]\n",
    "print(f\"Sequence level dataframe without aggregated info: {df_seqs.filter(pl.col('seq_id') == seq_id)}\")\n",
    "seq_enhancer = er.SequenceEnhancer(df = df, df_seq = df_seqs)\n",
    "df_seqs = seq_enhancer.events()\n",
    "print(f\"list of events in a sequence: {format_as_list(df_seqs.filter(pl.col('seq_id') == seq_id)['e_event_id'][0])}\")\n",
    "df_seqs = seq_enhancer.next_event_prediction()\n",
    "print(f\"Predicted events ngram:       {format_as_list(df_seqs.filter(pl.col('seq_id') == seq_id)['nep_predict'][0])}\")\n",
    "#df_seqs = seq_enhancer.eve_len()\n",
    "df_seqs = seq_enhancer.seq_len()\n",
    "print(f\"sequence length in events: {df_seqs.filter(pl.col('seq_id') == seq_id)['seq_len'][0]}\")\n",
    "df_seqs = seq_enhancer.start_time()\n",
    "df_seqs = seq_enhancer.end_time()\n",
    "df_seqs = seq_enhancer.duration()\n",
    "print(f\"sequence duration: {df_seqs.filter(pl.col('seq_id') == seq_id)['duration'][0]}\")\n",
    "df_seqs = seq_enhancer.tokens(token=\"e_trigrams\")\n",
    "df_seqs = seq_enhancer.tokens(token=\"e_words\")\n",
    "print(f\"Sequence level dataframe with aggregated info: {df_seqs.filter(pl.col('seq_id') == seq_id)}\")\n",
    "\n",
    "#_________________________________________________________________________________________\n",
    "#Part 5 Do some anomaly detection\n",
    "\n",
    "sad = ad.AnomalyDetection()\n",
    "sad.test_train_split (seq_enhancer.df_seq, test_frac=0.90)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# Choose the data what you use to train the model\n",
    "# ==============================================\n",
    "#if False:\n",
    "\n",
    "    \n",
    "'''\n",
    "\n",
    "    # ============================\n",
    "    # Use only one model at a time\n",
    "    # ============================\n",
    "\n",
    "    #Logistic Regression\n",
    "    sad.train_LR()\n",
    "    #df_seq = sad.predict() # we don't need the predictions now so they are removed\n",
    "    \n",
    "    #Use Decision Tree\n",
    "    # sad.train_DT()\n",
    "    #df_seq = sad.predict()\n",
    "\n",
    "if True:\n",
    "    print(f\"Predicting with words\")\n",
    "    sad.item_list_col = \"e_words\"\n",
    "    sad.numeric_cols = None #Important otherwise we use both numeric_col and item_list_col for predicting\n",
    "    sad.prepare_train_test_data() #Data needs to prepared after changing the predictor columns\n",
    "\n",
    "\n",
    "    # ============================\n",
    "    # Use only one model at a time\n",
    "    # ============================\n",
    "'''\n",
    "\n",
    "    #Logistic Regression\n",
    "print(f\"Predicting with words\")\n",
    "sad.item_list_col = \"e_words\"\n",
    "sad.numeric_cols = None #Important otherwise we use both numeric_col and item_list_col for predicting\n",
    "sad.prepare_train_test_data() #Data needs to prepared after changing the predictor columns\n",
    "X_train , labels_train = sad.train_data\n",
    "print(f\"train {X_train}\")\n",
    "ebm = LogisticRegression()\n",
    "ebm.fit(X_train, labels_train)\n",
    "ebm_global = ebm.explain_global()\n",
    "show(ebm_global)\n",
    "'''\n",
    "    #sad.train_LR()\n",
    "    #df_seq = sad.predict()     # we don't need the predictions now so they are removed\n",
    "\n",
    "    #Use Decision Tree\n",
    "    #sad.train_DT()\n",
    "    #df_seq = sad.predict()\n",
    "\n",
    "if False:\n",
    "    print(f\"Predicting with Drain parsing results\")\n",
    "    sad.item_list_col = \"e_event_id\"\n",
    "    sad.prepare_train_test_data()\n",
    "\n",
    "    # ============================\n",
    "    # Use only one model at a time\n",
    "    # ============================   \n",
    "     \n",
    "    #Logistic Regression\n",
    "    sad.train_LR()\n",
    "    #df_seq = sad.predict()\n",
    "    \n",
    "    #Use Decision Tree\n",
    "    #sad.train_DT()\n",
    "    #df_seq = sad.predict()\n",
    "\n",
    "\n",
    "# Above choose one model to be trained with wanted data\n",
    "\n",
    "# Gives the trained model\n",
    "modelout = sad.get_model\n",
    "\n",
    "# Get out the train data and the test data\n",
    "X_train , labels_train = sad.train_data\n",
    "X_test, labels_test = sad.test_data\n",
    "\n",
    "# Get out the vectorizer\n",
    "vect = sad.vec\n",
    "\n",
    "voc = sad.voc\n",
    "\n",
    "print(X_test.shape)\n",
    "#print(voc)\n",
    "#print(vect.get_feature_names_out())\n",
    "print(vect.get_feature_names_out().shape)\n",
    "#print(vect.vocabulary_)\n",
    "\n",
    "# for shap I needed the model and both train and test data\n",
    "explainer_ebm = shap.LinearExplainer(modelout, X_train)\n",
    "shap_values = explainer_ebm(X_test)\n",
    "#shap.summary_plot(shap_values, X_train, max_display=16)\n",
    "shap.plots.bar(shap_values)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
